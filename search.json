[
  {
    "objectID": "connect.html",
    "href": "connect.html",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "Connecting to the cluster with SSH must be done with an SSH client. Most operating systems have an SSH client installed by default.\n\n\n\n\n\n\nWarning\n\n\n\nWhen connecting to Mantis over SSH, you will be connected to a login node, also known as a head node. Please do not run anything on these nodes that is computationally demanding as this can negatively impact other users also connecting to these nodes. To do real work, submit a batch job or initiate an interactive session. Long running or intensive processes found running on login nodes may be killed without notice. This includes some installations such as when using Conda or compiling code. When in doubt, it is best to submit a batch job or initiate an interactive session.\n\n\n\n\n\n\nWindows users on Windows 10 or later can use the built-in SSH client through the Windows PowerShell.\nAlternatively, you can use a third-party SSH client such as MobaXterm.\n\n\n\nMacOS and Linux users can use the built-in SSH client through the Terminal application.\n\n\n\n\nTo connect to Mantis, open a Terminal or Windows Powershell window and enter the following command:\nssh &lt;user&gt;@&lt;host&gt;\nWhere &lt;user&gt; is your CAM username and host is one of the host URLs explained below.\n\n\nssh myusername@login.cam.hpc.edu\n\n\n\n\nThere are two host URLs you can use to connect to the Mantis cluster:\n\nAn internal host URL: mantis-submit.cam.uchc.edu, which requires you to be connected to the CAM VPN or to a Secure WiFi network on one of the UConn or UConn Health campuses.\nAn external host URL: login.hpc.cam.uchc.edu, which can be accessed from anywhere without a VPN connection, but requires that you have setup an SSH key for authentication.\n\n\n\n\n\nWe recommend using this host only for initial setup of your SSH keys for more secure authentication. This host is only accessible from the UConn or UCHC Secure WiFi networks or through the CAM VPN and requires password authentication.\nWhen connecting through this host, you will be redirected to one of 4 login nodes:\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu\nmantis-sub-4.cam.uchc.edu\n\n\nCAM passwords expire every 90 days and can be reset in the CAM Password Manager.\n\n\n\n\n\n\n\nThis is the recommended host URL for connecting to the Mantis cluster. It can be accessed from anywhere without a VPN connection, but requires that you have setup an SSH key for authentication. Authenticaion with SSH keys is more secure than password authentication as well as being more convenient.\nWhen connecting through this host, you will be redirected to one of 3 login nodes:\n\nmantis-sub-5.cam.uchc.edu\nmantis-sub-6.cam.uchc.edu\nmantis-sub-7.cam.uchc.edu",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#ssh-clients",
    "href": "connect.html#ssh-clients",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "Windows users on Windows 10 or later can use the built-in SSH client through the Windows PowerShell.\nAlternatively, you can use a third-party SSH client such as MobaXterm.\n\n\n\nMacOS and Linux users can use the built-in SSH client through the Terminal application.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#using-ssh",
    "href": "connect.html#using-ssh",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "To connect to Mantis, open a Terminal or Windows Powershell window and enter the following command:\nssh &lt;user&gt;@&lt;host&gt;\nWhere &lt;user&gt; is your CAM username and host is one of the host URLs explained below.\n\n\nssh myusername@login.cam.hpc.edu",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#host-urls",
    "href": "connect.html#host-urls",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "There are two host URLs you can use to connect to the Mantis cluster:\n\nAn internal host URL: mantis-submit.cam.uchc.edu, which requires you to be connected to the CAM VPN or to a Secure WiFi network on one of the UConn or UConn Health campuses.\nAn external host URL: login.hpc.cam.uchc.edu, which can be accessed from anywhere without a VPN connection, but requires that you have setup an SSH key for authentication.\n\n\n\n\n\nWe recommend using this host only for initial setup of your SSH keys for more secure authentication. This host is only accessible from the UConn or UCHC Secure WiFi networks or through the CAM VPN and requires password authentication.\nWhen connecting through this host, you will be redirected to one of 4 login nodes:\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu\nmantis-sub-4.cam.uchc.edu\n\n\nCAM passwords expire every 90 days and can be reset in the CAM Password Manager.\n\n\n\n\n\n\n\nThis is the recommended host URL for connecting to the Mantis cluster. It can be accessed from anywhere without a VPN connection, but requires that you have setup an SSH key for authentication. Authenticaion with SSH keys is more secure than password authentication as well as being more convenient.\nWhen connecting through this host, you will be redirected to one of 3 login nodes:\n\nmantis-sub-5.cam.uchc.edu\nmantis-sub-6.cam.uchc.edu\nmantis-sub-7.cam.uchc.edu",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#creating-a-public-private-key-pair.",
    "href": "connect.html#creating-a-public-private-key-pair.",
    "title": "Connecting to Mantis",
    "section": "Creating a public private key pair.",
    "text": "Creating a public private key pair.\nYou can create an SSH key pair using the SSH client.\n\nTo create an SSH key pair to use with the cluster, use the following command:\n\nssh-keygen -t ed25519 -C &lt;comment&gt;\n\nThe comment should be something to help you identify which computer this key comes from. For example &lt;user&gt;-laptop\nYou will be prompted to specify the location to save the key. You can press enter to accept the default location.\nNext you will be prompted to create a passphrase which is optional. If you don’t wish to create a passphrase, simply press enter.\n\n\nCopy to Mantis\nAfter creating a key pair or if you have an existing one that you want to use, you can copy it to the cluster.\n\nMacOS and LinuxWindows PowerShell\n\n\nOn MacOS and Linux, you can copy your public key to the Mantis cluster using the ssh-copy-id command:\nssh-copy-id &lt;user&gt;@mantis-submit.cam.uchc.edu\nWhere &lt;user&gt; is your Mantis username.\n\n\nOn Windows 10 or later, you can copy your public key to the Mantis cluster with Windows PowerShell using the command:\ntype $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh &lt;user&gt;@mantis-submit.cam.uchc.edu \"cat &gt;&gt; .ssh/authorized_keys\"\nWhere &lt;user&gt; is your Mantis username.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "Getting Help\nIf you have a question that is not answered on this site, please submit a ticket here: https://bioinformatics.uconn.edu/contact-us/bioinformatics-technical-issues/\n\n\nSlack\nJoin the cbc slack channel to ask questions and receive updates and notifications.",
    "crumbs": [
      "Get Help"
    ]
  },
  {
    "objectID": "quick.html",
    "href": "quick.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Accessible on campus or through VPN.\n\n\nmantis-submit.cam.uchc.edu\n\n\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#internal-login-node",
    "href": "quick.html#internal-login-node",
    "title": "UCHC HPC",
    "section": "",
    "text": "Accessible on campus or through VPN.\n\n\nmantis-submit.cam.uchc.edu\n\n\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#external-login-node",
    "href": "quick.html#external-login-node",
    "title": "UCHC HPC",
    "section": "External Login Node",
    "text": "External Login Node\nRequires SSH key.\nhpc.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#transfer-node",
    "href": "quick.html#transfer-node",
    "title": "UCHC HPC",
    "section": "Transfer Node",
    "text": "Transfer Node\ntransfer.cam.uchc.edu"
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Hardware\nOperating System\nStatistics\nCurrent Status?\nCurrent Usage?\nAcknowledgements\nMaybe move all of this to introduction?"
  },
  {
    "objectID": "submission.html",
    "href": "submission.html",
    "title": "Job Submission",
    "section": "",
    "text": "The submit nodes on the cluster should only be used for a limited range of tasks that do not have high computational demands. See the best practices page for more information on appropriate use of the submit nodes. When in doubt it is better to complete a task as a job on a compute node.\nThe UCHC Computer cluster uses SLURM for managing and scheduling jobs.\nWith SLURM there are two primary ways that a job can be run on a compute node.",
    "crumbs": [
      "Submit Jobs"
    ]
  },
  {
    "objectID": "submission.html#general",
    "href": "submission.html#general",
    "title": "Job Submission",
    "section": "general",
    "text": "general\nThis partition contains nodes suitable for most tasks. These nodes have 64-128 AMD or Intel Xeon CPUs and 192 GB - 512 GB of ram. Most nodes in this partition have either one NVIDIA A100 or three NVIDIA L40S GPUs.",
    "crumbs": [
      "Submit Jobs"
    ]
  },
  {
    "objectID": "submission.html#himem",
    "href": "submission.html#himem",
    "title": "Job Submission",
    "section": "himem",
    "text": "himem\nThis parition contains nodes suitable for jobs that have very large memory requirements. These nodes have 128 AMD CPUs and 1 TB of ram. Each node in this partition has one NVIDIA A100 GPU.",
    "crumbs": [
      "Submit Jobs"
    ]
  },
  {
    "objectID": "submission.html#debug",
    "href": "submission.html#debug",
    "title": "Job Submission",
    "section": "debug",
    "text": "debug\nThis partition is intended for testing and debugging jobs. It has a time limit of only 5 minutes but jobs should start very quickly, allowing you to check for errors before submitting to a queue with potentially longer wait times.",
    "crumbs": [
      "Submit Jobs"
    ]
  },
  {
    "objectID": "submission.html#preemtable",
    "href": "submission.html#preemtable",
    "title": "Job Submission",
    "section": "preemtable",
    "text": "preemtable\nJobs run in this partition my be preempted by higher priority users. This partition is intended for users who are willing to have their jobs interrupted in exchange for potentially lower wait time when not in use by higher priority users. Jobs with short run times or jobs that can be restartd are good candidates for this partition.",
    "crumbs": [
      "Submit Jobs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UConn Health High Performance Computing",
    "section": "",
    "text": "This site provides documentation for users of the Mantis computer cluster at UConn Health. The Mantis cluster is available to all UConn and UConn health researchers.\nThis page is maintained by the Computational Biology Core in the Institute for Systems Genomics.\nThere is a second high performance computing facility (also available to all UConn researchers) located at the Storrs campus. To learn more about accessing and using that system, visit this site."
  },
  {
    "objectID": "storage.html",
    "href": "storage.html",
    "title": "File systems and storage",
    "section": "",
    "text": "The HPC offers a variety of systems for storage and archiving of data. With a few noted exceptions, all systems below are network-attached, meaning they are accessible from all compute and login nodes.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#active-work-spaces",
    "href": "storage.html#active-work-spaces",
    "title": "File systems and storage",
    "section": "Active work spaces",
    "text": "Active work spaces\nThe following spaces are suitable for use for active data analysis.\n\n/home/FCAM/&lt;user&gt;\nQuota: 25GB. Generally cannot be expanded.\nAll users will have a home directory when their account is created.\nThis location is primarily intended for storage of small files such as scripts, source code, configuration, software installations, and can accommodate a small amount of data.\nUse other spaces for analysis of large datasets.\n\n\n\n\n\n\nNote\n\n\n\nUsers are not permitted to open permissions on their home directories. If you need to share files with other users, try any of the below directories you have access to. If you open permissions on your home directory, it may be locked down.\n\n\n\n\n/labs\nQuota: 2TB. can be increased upon request.\nIndividual users will always be associated with “lab” groups. PI of the lab group can request a space in /labs.\nThis location is intended for storage of active shared project data to facilitate collaboration among members of a PI’s lab. You will need to be part of the PI’s permission group to access a lab directory.\n\n\n/projects\nQuota: 2TB, can be increased upon request.\nThis can be created at the request of one or more PIs.\nThis location is intended for storage of active shared project data to facilitate collaboration among multiple groups on a project.\n\n\n/scratch\nQuota: Shared space. No individual or group quotas.\nAnyone can create any number of directories here and set permissions to whatever they choose.\nThis location is intended to serve as temporary storage. This would be an ideal place for the output of intermediate files during analysis. As of last update, /scratch had 84TB of space. Please be considerate of other users of this shared resource.\n\n\n\n\n\n\nWarning\n\n\n\nIn /scratch unused files are subject to deletion without notice after 90 days.\n\n\n\n\n/sandbox\nQuota: Shared space. No individual or group quotas.\nThis system has identical policies to /scratch. It is in development. At the time of writing (7/25) it was not up.\n\n\n/seqdata\nQuota: Shared space. No individual or group quotas. Users do not have write access.\nThis location is used for storing raw sequencing data and is intended to alleviate strain on quotas in other locations and to discourage data duplication. Data are stored as read-only. You may request that raw data be stored here (all data generated by CGI is stored here by default), but CBC will have to move it there for you, as users do not have write access.\nIf you would like to move existing raw data to this location or store data from an external sequencing center, please contact us at cbcsupport@helpspotmail.com.\nDo not copy data from this directory. For convenience, consider making a symlink to files stored here. By avoiding unnecessary copying, accidental data corruption or deletion can be avoided and space will be used more efficiently on the cluster and within directories you own.\n\n\n\n\n\n\nTip\n\n\n\nInstead of copying data from /seqdata, create a symlink.\nA symlink, or symbolic link is a pointer to a file or directory that behaves like the real thing.\nIf you want to use data from /seqdata as if it were in one of your project directories you can create a symlink like this:\nln -s /seqdata/CGI/Fastq_Files/AwesomeWGS_May2025/ /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/\nWhere /seqdata/CGI/Fastq_Files/AwesomeWGS_May2025 is the raw data directory and /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/ is the destination. You will then have a symlink, /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/AwesomeWGS_May2025 that behaves just as if the files contained had been copied without taking up all the space.\n\n\n\n\n/tmp\nQuota: Shared space. No individual or group quotas.\nEach node has it’s own local /tmp directory which can be used for temporary storage of data while actively running analyses. This can be desirable when an analysis would be limited by I/O operations across the network.\n/tmp is a shared space. Once an analysis is done, nothing should be left behind here.\n\n\n\n\n\n\nTip\n\n\n\nOn our system /tmp is pretty small and often fills up. Many programs quietly write to /tmp by default without telling users (they usually quietly clean up afterward as well). If /tmp fills up, no space left on device errors frequently result, confusing users who know they still have plenty of space in their storage quota. If you are running a program that does this, say:\ngenomeAssembler -i mySequences.fastq.gz -o myGenome.fasta\nYou can usually get it to write to another temporary directory by setting and exporting the variable TMPDIR to somewhere else before running it:\nexport TMPDIR=myNewTmpDir # this directory must exist or an error will result\n\ngenomeAssembler -i mySequences.fastq.gz -o myGenome.fasta\nYou can create your own temporary directory where you are doing the analysis, or you can use /local.\n\n\nThe system will remove contents of this directory after TODO: how long or under what conditions?\n\n\n/local\nQuota: Shared space. No individual or group quotas.\nThis is another space local to each node and shared among users. It is larger than /tmp but files should be removed immediately after analysis.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#archival-spaces",
    "href": "storage.html#archival-spaces",
    "title": "File systems and storage",
    "section": "Archival spaces",
    "text": "Archival spaces\nThese spaces are meant for archiving data for varying durations. Users can request the creation of directories they have read/write access to.\n\n/archive\nQuota: No current quota policy.\nThis system is intended for medium term storage of data that is no longer being used for analysis, but which may be needed within a year or so. /archive gives relatively fast access (though not as fast as the above directories). The data are securely stored, being geospread across four data centers. As such, it is relatively expensive for us to store data here. Users should not plan on permanently archiving data here.\nPIs and users can request storage space here in a directory structure that mirrors /home/FCAM, /projects and /labs to which they have read/write access.\nEventually, data stored here will be moved to tape storage. TODO: Say a bit more about the goal of archive and tape. How safe is data here? Does it need to also be stored elsewhere such as SRA?\n\n\n/tapebackup\nQuota: No current quota policy.\nThis system is intended for short term storage (&lt; 1 year). Think of it as a temporary storage space for files you don’t currently need, but are not ready to delete.\nThis system stores data on magnetic tape. Data stored here goes on a single magnetic tape and is not backed up, though magnetic tape is highly stable.\nAccess to magnetic tape is slow. Depending on how busy the system is, it may take hours or days for data to be written or retrieved. It is, however, inexpensive.\nThe directory structure is similar to /archive. Users can request space here to which they have read/write access.\n\n\n/tapearchive\nQuota: No current quota policy.\nThis system is intended for long term storage. Completed projects and datasets that are no longer currently active can be stored here.\nIt also stores data on magnetic tape, but on two redundant systems. Like /tapebackup, access is slow.\nUser access and directory structure are the same as /archive and /tapebackup.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#summary-table",
    "href": "storage.html#summary-table",
    "title": "File systems and storage",
    "section": "Summary table",
    "text": "Summary table\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD | Directory | Purpose | Access | Quota/Capacity | Notes | |———————-|——————————————————–|————————————-|————————-|—————————————————————————-| | /home/FCAM/&lt;user&gt; | Personal home directory for code, config, small data | Created for each user | 1TB (increasable) | Permissions must remain private. For sharing, use /labs or /projects. | | /labs | Shared lab project space | PI request, lab group members | 2TB (increasable) | For intra-lab collaboration. | | /projects | Shared inter-lab project space | PI request, project group members | 2TB (increasable) | For collaboration across labs. | | /scratch | Temporary storage for active analyses | Open to all users | 84TB (shared) | Files may be deleted after 90 days. | | /sandbox | Temporary space (in development) | Same as /scratch | TBD | Not yet active as of 7/25. | | /seqdata | Central store for raw/original sequencing data | Read-only for users | NA | Use symlinks instead of copying. Contact CBC to store new data. | | /tmp | Node-local temporary space | Shared per node | Small (varies by node) | May fill up unexpectedly. Set TMPDIR to redirect temp files. | | /local | Larger node-local temp space | Shared per node | Larger than /tmp | Clean up after use. | | /archive | Medium-term storage (up to ~1 year) | Request via CBC, access varies | NA | Slower than active spaces, geospread, more expensive. | | /tapebackup | Long-term storage (1+ years) | Request via CBC, access varies | NA | Stored on single magnetic tape, not redundant, very slow access. | | /tapearchive | Very long-term or permanent storage | Request via CBC, access varies | NA | Redundant magnetic tape storage, very slow access. | ======= | Directory | Purpose | Access | Quota/Capacity | Notes | |—————————|——————————————————–|—————————————–|———————————————|———————————————————————————————| | /home/FCAM/&lt;user&gt; | Personal home directory for code, config, small data | Created for each user | 25GB | Permissions must remain private. For sharing, use /labs or /projects. | | /labs | Shared lab project space | PI request, lab group members | 2TB (increasable) | For intra-lab collaboration. | | /projects | Shared inter-lab project space | PI request, project group members | 2TB (increasable) | For collaboration across labs. | | /scratch | Temporary storage for active analyses | Open to all users | 84TB (shared) | Files may be deleted after 90 days. | | /sandbox | Temporary space (in development) | Same as /scratch | TBD | Not yet active as of 7/25. | | /seqdata | Central store for raw/original sequencing data | Read-only for users | NA | Use symlinks instead of copying. Contact CBC to store new data. | | /tmp | Node-local temporary space | Shared per node | Small | May fill up unexpectedly. Set TMPDIR to redirect temp files. | | /local | Larger node-local temp space | Shared per node | Larger than /tmp | Clean up after use. | | /archive | Medium-term storage (up to ~1 year) | Request via CBC, access varies | NA | Slower than active spaces, geospread, more expensive. | | /tapebackup | Long-term storage (1+ years) | Request via CBC, access varies | NA | Stored on single magnetic tape, not redundant, very slow access. | | /tapearchive | Very long-term or permanent storage | Request via CBC, access varies | NA | Redundant magnetic tape storage, very slow access. | &gt;&gt;&gt;&gt;&gt;&gt;&gt; 457542131094390cb510c04fe1e4b66dcb413b48",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#projects-1",
    "href": "storage.html#projects-1",
    "title": "File systems and storage",
    "section": "/projects",
    "text": "/projects\nThis can be created at the request of a PI.\nThis location is intended for storage of active shared project data to facilitate collaboration among multiple labs on a project.\nQuota: 2TB, can be increased upon request.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#scratch-1",
    "href": "storage.html#scratch-1",
    "title": "File systems and storage",
    "section": "/scratch",
    "text": "/scratch\nAnyone can create a directory here.\nThis location is intended to serve as temporary storage. This would be an ideal place for the output of intermediate files during analysis.\nFiles may be removed from this location after 90 days.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#sandbox-1",
    "href": "storage.html#sandbox-1",
    "title": "File systems and storage",
    "section": "/sandbox",
    "text": "/sandbox",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#seqdata-1",
    "href": "storage.html#seqdata-1",
    "title": "File systems and storage",
    "section": "/seqdata",
    "text": "/seqdata\nThis location is used for storing raw and original sequencing data. Data are stored as read-only.\nDo not copy data from this directory. For convenience, consider making a symlink to files stored here. By avoiding unneccessary copying, accidental data corruption or deletion can be avoided and space will be used more efficiently on the cluster and within directories you own.\nIf you would like to move existing raw data to this location or store data from an external sequencing center, please contact us at cbcsupport@helpspotmail.com.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#archive-1",
    "href": "storage.html#archive-1",
    "title": "File systems and storage",
    "section": "/archive",
    "text": "/archive\nThis location is intended for long term storage of data that is no longer being used for analysis. Eventually, data stored here will be moved to tape storage. TODO: Say a bit more about the goal of archive and tape. How safe is data here? Does it need to also be stored elsewhere such as SRA?\nIf you would like to move data to this location please contact us at cbcsupport@helpspotmail.com\n\nReading from this directory is slow compared to /seqdata. Request that data be moved back to /seqdata if you will be actively using the data for analysis.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#tmp-1",
    "href": "storage.html#tmp-1",
    "title": "File systems and storage",
    "section": "/tmp",
    "text": "/tmp\nEach node has it’s own local /tmp directory which can be used for temporary storage of data while running analyses. This can be desirable when an analysis would be limited by I/O operations across the network.\nOnce an analysis is done, important data need to be moved out of this space and any other files should be removed.\nThe system will remove contents of this directory after TODO: how long or under what conditions?.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "transfer.html",
    "href": "transfer.html",
    "title": "Tranferring Data",
    "section": "",
    "text": "We recommend two approaches for transferring data to and from the cluster.\nEach are most suitable for different situations. We explain the usage of each tool below.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#secure-copy",
    "href": "transfer.html#secure-copy",
    "title": "Tranferring Data",
    "section": "Secure Copy",
    "text": "Secure Copy\nTODO: Are all network storage filesystems mounted to the transfer nodes?\n\nSecure copy is ideal for transferring small amounts of data and is a bit more straight forward to use than Globus connect. A secure copy can be performed with the scp command. This command can only be used to transfer data to and from a transfer node. There are two transfer nodes.\n\ntransfer.cam.uchc.edu This will accept password authentication or ssh keys and is available from anywhere.\ntransfer.hpc.cam.uchc.edu This is actually three load-balanced servers. it will only accept SSH key authentication and can only be reached through the CAM VPN.\n\nA secure copy can be performed from the terminal with the scp command which has this required syntax:\nscp &lt;src&gt; &lt;dest&gt;\nThe src and dest arguments specify file or directory paths.\nAny remote file path must be prefixed with a user ID along with the transfer node address and separated from the path with a colon like so:\n&lt;user&gt;@&lt;hostname&gt;:.\nIn order to secure copy a directory use the -r option.\n\nExamples\nCopy a file from your local computer to your home directory on the cluster:\nscp &lt;local path&gt; &lt;user&gt;@transfer.cam.uchc.edu:&lt;remote path&gt;\nCopy a directory from the cluster to your local computer:\nscp -r &lt;user&gt;@transfer.cam.uchc.edu:&lt;remote path&gt; &lt;local path&gt;",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#globus-connect",
    "href": "transfer.html#globus-connect",
    "title": "Tranferring Data",
    "section": "Globus Connect",
    "text": "Globus Connect\nGlobus Connect is well suited for large file transfers and for transferring between institutions such as when you need to share files with a collaborator.\n\nConnecting with Globus Connect\n\nLog in to the Globus web app at https://www.globus.org.\nSelect the University of Connecticut as your organization and enter your login credentials when prompted.\nOnce logged in, connect to UConn HPC by searching for “UCHC Globus Server” in the collection text field.\nSelect UCHC Globus Server and log in with your CAM credentials.\n\n\n\n\n\n\n\nNote\n\n\n\nGlobus Connect also has a desktop client available for installation on your local computer.\nThis can be downloaded at https://www.globus.org/globus-connect-personal\n\n\n\n\nFinding files in UCHC Globus Server collection\nThere are two root directories within the globus server collection. You can navigate the server file system by entering paths within these two directories into the path text field.\n\n/globus\nThis Globus file server directory contains many of the directories located in root directory of the cluster file system. From here you can access the /labs, /projects, or /seqdata cluster file system directories among others.\n\n\n/home\nThis Globus file server directory contains the /home cluster file system directory. You would access your home directory by navigating to /home/FCAM/&lt;user&gt; in the Globus file server.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "account.html",
    "href": "account.html",
    "title": "Requesting an Account",
    "section": "",
    "text": "Requesting an Account\nTo request a Mantis account: 1. Submit an account request ticket. 2. Select New Account Request from the Request Type dropdown menu. 3. Choose HPC Cluster Account as the Account Type. 4. Fill in all relevant and required fields and click submit.\nOnce your request is approved, you will receive an email with your account information. This will include a username which is different from your NetID and a temporary password.\n\n\nPasswords\nUpon approval of an account request, you will need to enroll in the CAM Password Manager and reset your password.\nPasswords expire every 90 days.\nAny time you need to reset your password, you can do so in the CAM Password Manager.\n\nWhy is it called the CAM Password Manager?\nMuch of the administration of the Mantis cluster is handled by staff in the Cell Analysis and Modeling (CAM) department.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are not able to login or transfer files, ensure that you don’t have an expired password.",
    "crumbs": [
      "Account & Password"
    ]
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "Databases Available on Mantis",
    "section": "",
    "text": "We host several commonly used bioinformatics databases on the Mantis cluster. We update these regularly: every three months for databases sourced from NCBI, EMBL-EBI, and SIB repositories and every six months for all other databases. We aim to host the latest version and previous version of each database at all times. We also host several user-made databases where there is demand.",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#blast",
    "href": "databases.html#blast",
    "title": "Databases Available on Mantis",
    "section": "Blast",
    "text": "Blast\nThe latest versions of NCBI’s BLAST databases are available on Mantis. This directory also includes custom databases constructed from the newest eggNOG, orthoDB, and UniProt releases. All databases are compatible with BLAST+ v2.7.1 and up. You may use them with any blast command requiring a database (e.g., blastn, blastp).\n\nExample:\nblastp -db &lt;blast database path&gt; &lt;other options&gt;\n\n\nPaths to available databases\n\n/isg/shared/databases/blast/v4/\n/isg/shared/databases/blast/v5/\n/isg/shared/databases/blast/eggnog/v5.0.1/\n/isg/shared/databases/blast/eggnog/v5.0.2/\n/isg/shared/databases/blast/orthodb/v12v1/\n/isg/shared/databases/blast/uniprot/v2025.06/",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#busco",
    "href": "databases.html#busco",
    "title": "Databases Available on Mantis",
    "section": "BUSCO",
    "text": "BUSCO\nThe latest versions of the EZLab’s BUSCO databases are available on Mantis. This directory includes all lineage-specific odb10 and odb12 databases, compatible with BUSCO v5.0.0 and up.\n\nExample:\nbusco -l &lt;busco database path&gt; &lt;other options&gt;\n\n\nPaths to available databases\n\n/isg/shared/databases/busco/odb10/\n/isg/shared/databases/busco/odb12/",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#cellranger",
    "href": "databases.html#cellranger",
    "title": "Databases Available on Mantis",
    "section": "CellRanger",
    "text": "CellRanger\nThe latest versions of 10X Genomics’ CellRanger databases are available on Mantis. This directory contains human, mouse, and rat genomic and transcriptomic reference data, compatible with all versions of CellRanger.\n\nExample:\ncellranger count --transcriptome &lt;cellranger database path&gt; &lt;other options&gt;\n\n\nPaths to available databases\n\n/isg/shared/databases/cellranger/v2020A/\n/isg/shared/databases/cellranger/v2024A/",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Intro command line shell scripting\nFrequently used commands\nMove file permissions here as a subcategory?"
  },
  {
    "objectID": "permissions.html",
    "href": "permissions.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "File permissions"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Environment Modules",
    "section": "",
    "text": "TODO: Write intro here\n\nEnvironment Modules\nInstalled software is managed with Environment modules.\nSee what software is available: module avail\nLoad software into environment:\nmodule load &lt;software&gt;\n\n\nSingularity\nTODO: How to use a singularity continer.\n\n\nConda\nTODO: How to install and use Conda.\n\n\nRequest Software\nUsers can request the installation of software by submitting a software request",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "bestpractices.html",
    "href": "bestpractices.html",
    "title": "Data storage practices",
    "section": "",
    "text": "How to be a good citizen",
    "crumbs": [
      "Best Practices"
    ]
  },
  {
    "objectID": "bestpractices.html#sharing-data-with-other-users",
    "href": "bestpractices.html#sharing-data-with-other-users",
    "title": "Data storage practices",
    "section": "Sharing data with other users",
    "text": "Sharing data with other users\nUsers often need to share files with others, or work collaboratively in a directory. To do this, you need to set appropriate permissions. You are not permitted to open permissions for other users to your home directory /home/FCAM/&lt;user&gt;. If you do this, your directory may be locked down and you may temporarily lose access to HPC resources. To share or collaborate on work, use a directory in /labs, /projects, or /scratch that you have access to.\nDon’t run things on login nodes - conda - file compression - large compilations\nTest before scaling\nWarning about LLMs. Mainly, don’t blindly run output from them.",
    "crumbs": [
      "Best Practices"
    ]
  },
  {
    "objectID": "snapshots.html",
    "href": "snapshots.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Instructions on recovering files from .snapshots"
  }
]