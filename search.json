[
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "Databases Available on Mantis",
    "section": "",
    "text": "We host several commonly used bioinformatics databases on the Mantis cluster. We update these regularly—every three months for databases sourced from NCBI, EMBL-EBI, and SIB repositories and every six months for all other databases. We aim to host the latest version and previous version of each database at all times. We also host several user-made databases where there is demand.",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#blast",
    "href": "databases.html#blast",
    "title": "Databases Available on Mantis",
    "section": "Blast",
    "text": "Blast\nThe latest versions of NCBI’s BLAST databases are available on Mantis. This directory also includes custom databases constructed from the newest eggNOG, orthoDB, and UniProt releases. All databases are compatible with BLAST+ v2.7.1 and up. You may use them with any blast command requiring a database (e.g., blastn, blastp).\n\nExample:\nblastp -db &lt;blast database path&gt; &lt;other options&gt;\n\n\nPaths to available databases\n\n/isg/shared/databases/blast/v4/\n/isg/shared/databases/blast/v5/\n/isg/shared/databases/blast/eggnog/v5.0.1/\n/isg/shared/databases/blast/eggnog/v5.0.2/\n/isg/shared/databases/blast/orthodb/v12v1/\n/isg/shared/databases/blast/uniprot/v2025.06/",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#busco",
    "href": "databases.html#busco",
    "title": "Databases Available on Mantis",
    "section": "BUSCO",
    "text": "BUSCO\nThe latest versions of the EZLab’s BUSCO databases are available on Mantis. This directory includes all lineage-specific odb10 and odb12 databases, compatible with BUSCO v5.0.0 and up.\n\nExample:\nbusco -l &lt;busco database path&gt; &lt;other options&gt;\n\n\nPaths to available databases\n\n/isg/shared/databases/busco/odb10/\n/isg/shared/databases/busco/odb12/",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "databases.html#cellranger",
    "href": "databases.html#cellranger",
    "title": "Databases Available on Mantis",
    "section": "CellRanger",
    "text": "CellRanger\nThe latest versions of 10X Genomics’ CellRanger databases are available on Mantis. This directory contains human, mouse, and rat genomic and transcriptomic reference data, compatible with all versions of CellRanger.\n\nExample:\ncellranger count --transcriptome &lt;cellranger database path&gt; &lt;other options&gt;\n\n\nPaths to available databases\n\n/isg/shared/databases/cellranger/v2020A/\n/isg/shared/databases/cellranger/v2024A/",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UConn Health High Performance Computing",
    "section": "",
    "text": "This site provides documentation for users of the Mantis computer cluster at UConn Health. The Mantis cluster is a high performance computing (HPC) system available to all UConn and UConn Health researchers. It is primarily used for bioinformatics and computational biology applications.\nThis page is maintained by the University of Connecticut Computational Biology Core in the Institute for Systems Genomics.\nAdministration of the Mantis cluster is jointly handled by the Center for Cell Analysis and Modeling at UConn Health and the Computational Biology Core.\nThere is a second high performance computing facility (also available to all UConn researchers) located at the Storrs campus. To learn more about accessing and using that system, visit this site."
  },
  {
    "objectID": "account.html",
    "href": "account.html",
    "title": "Requesting an Account",
    "section": "",
    "text": "Requesting an Account\nIn order to use the Mantis cluster, you must request a CAM account. It is referred to as a CAM account because much of the administration of the Mantis cluster, in addition to other systems, is handled by the Center for Cell Analysis and Modeling at UConn Health.\nTo request a CAM account:\n\nSubmit an account request ticket.\nSelect New Account Request from the Request Type dropdown menu.\nChoose HPC Cluster Account as the Account Type.\nFill in all relevant and required fields and click submit.\n\nOnce your request is approved, you will receive an email with your account information. This will include a username which is different from your NetID and a temporary password.\n\n\nPasswords\nUpon approval of an account request, you will need to enroll in the CAM Password Manager and reset your password.\nPasswords expire every 90 days.\nAny time you need to reset your password, you can do so in the CAM Password Manager.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are ever not able to login or transfer files, ensure that you don’t have an expired password.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not share your password with anyone! We will never ask you for your password for any reason.",
    "crumbs": [
      "Account & Password"
    ]
  },
  {
    "objectID": "storage.html",
    "href": "storage.html",
    "title": "File systems and storage",
    "section": "",
    "text": "The HPC offers a variety of systems for storage and archiving of data. With a few noted exceptions, all systems below are network-attached, meaning they are accessible from all compute and login nodes.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#active-work-spaces",
    "href": "storage.html#active-work-spaces",
    "title": "File systems and storage",
    "section": "Active work spaces",
    "text": "Active work spaces\nThe following spaces are suitable for use for active data analysis.\n\n/home/FCAM/&lt;user&gt;\nQuota: 25GB. Generally cannot be expanded.\nAll users will have a home directory when their account is created.\nThis location is primarily intended for storage of small files such as scripts, source code, configuration, software installations, and can accommodate a small amount of data.\nUse other spaces for analysis of large datasets.\n\n\n\n\n\n\nNote\n\n\n\nUsers are not permitted to open permissions on their home directories. If you need to share files with other users, try any of the below directories you have access to. If you open permissions on your home directory, it may be locked down.\n\n\n\n\n/labs\nQuota: 2TB. Can be increased upon request.\nIndividual users will always be associated with “lab” groups. PI of the lab group can request a space in /labs.\nThis location is intended for storage of active shared project data to facilitate collaboration among members of a PI’s lab. You will need to be part of the PI’s permission group to access a lab directory.\n\n\n/projects\nQuota: 2TB. Can be increased upon request.\nThis can be created at the request of one or more PIs.\nThis location is intended for storage of active shared project data to facilitate collaboration among multiple groups on a project.\n\n\n/scratch\nQuota: Shared space. No individual or group quotas.\nAnyone can create any number of directories here and set permissions to whatever they choose.\nThis location is intended to serve as temporary storage. This would be an ideal place for the output of intermediate files during analysis. As of last update, /scratch had 84TB of space. Please be considerate of other users of this shared resource.\n\n\n\n\n\n\nWarning\n\n\n\nIn /scratch unused files are subject to deletion without notice after 90 days.\n\n\n\n\n/sandbox\nQuota: Shared space. No individual or group quotas.\nThis system has identical policies to /scratch. It is in development. At the time of writing (7/25) it was not up.\n\n\n/seqdata\nQuota: Shared space. No individual or group quotas. Users do not have write access.\nThis location is used for storing raw sequencing data and is intended to alleviate strain on quotas in other locations and to discourage data duplication. Data are stored as read-only. You may request that raw data be stored here (all data generated by CGI is stored here by default), but CBC will have to move it there for you, as users do not have write access.\nIf you would like to move existing raw data to this location or store data from an external sequencing center, please contact us at cbcsupport@helpspotmail.com.\nRaw sequence data will be stored here for a period of 2 years. After this time, the data will be moved to /tapearchive/seqdata. See below for more information on /tapearchive.\nDo not copy data from this directory. For convenience, consider making a symlink to files stored here. By avoiding unnecessary copying, accidental data corruption or deletion can be avoided and space will be used more efficiently on the cluster and within directories you own.\n\n\n\n\n\n\nTip\n\n\n\nInstead of copying data from /seqdata, create a symlink.\nA symlink, or symbolic link is a pointer to a file or directory that behaves like the real thing.\nIf you want to use data from /seqdata as if it were in one of your project directories you can create a symlink like this:\nln -s /seqdata/CGI/Fastq_Files/AwesomeWGS_May2025/ /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/\nWhere /seqdata/CGI/Fastq_Files/AwesomeWGS_May2025 is the raw data directory and /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/ is the destination. You will then have a symlink, /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/AwesomeWGS_May2025 that behaves just as if the files contained had been copied without taking up all the space.\n\n\n\n\n/tmp\nQuota: Shared space. No individual or group quotas.\nEach node has it’s own local /tmp directory which can be used for temporary storage of data while actively running analyses. This can be desirable when an analysis would be limited by I/O operations across the network.\n/tmp is a shared space. Once an analysis is done, nothing should be left behind here.\n\n\n\n\n\n\n\nTip\n\n\n\nOn our system /tmp is pretty small and often fills up. Many programs quietly write to /tmp by default without telling users (they usually quietly clean up afterward as well). If /tmp fills up, no space left on device errors frequently result, confusing users who know they still have plenty of space in their storage quota. If you are running a program that does this, say:\ngenomeAssembler -i mySequences.fastq.gz -o myGenome.fasta\nYou can usually get it to write to another temporary directory by setting and exporting the variable TMPDIR to somewhere else before running it:\nexport TMPDIR=myNewTmpDir # this directory must exist or an error will result\n\ngenomeAssembler -i mySequences.fastq.gz -o myGenome.fasta\nYou can create your own temporary directory where you are doing the analysis, or you can use /local.\n\n\n\n\n/local\nQuota: Shared space. No individual or group quotas.\nThis is another space local to each node and shared among users. It is larger than /tmp but files should be removed immediately after analysis.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#archival-spaces",
    "href": "storage.html#archival-spaces",
    "title": "File systems and storage",
    "section": "Archival spaces",
    "text": "Archival spaces\nThese spaces are meant for archiving data for varying durations. Users can request the creation of directories they or a group of users will have read/write access to.\nEach of the archival spaces have the same directory structure containing the following directories:\n\n\n\n\n\n\n\nusers\nOwned by users\n\n\nlabs\nShared by users of a PI’s lab\n\n\nprojects\nShared by users belonging to multiple lab groups collaborating on a specific project\n\n\ndepartments\nShared by users of a department\n\n\n\n\n/archive\nQuota: No current quota policy.\nThis system is intended for medium term storage of data that is no longer being used for analysis, but which may be needed within a year or so. /archive gives relatively fast access (though not as fast as the above directories). The data are securely stored, being geospread across four data centers. As such, it is relatively expensive for us to store data here. Users should not plan on permanently archiving data here.\nEventually, data stored here will be moved to tape storage.\n\n\n/tapebackup\nQuota: No current quota policy.\nThis system is intended for short term storage (&lt; 1 year). Think of it as a temporary storage space for files you don’t currently need, but are not ready to delete.\nThis system stores data on magnetic tape. Data stored here goes on a single magnetic tape and is not backed up, though magnetic tape is highly stable.\nAccess to magnetic tape is slow. Depending on how busy the system is, it may take hours or days for data to be written or retrieved. It is, however, inexpensive.\n\n\n/tapearchive\nQuota: No current quota policy.\nThis system is intended for long term storage. Completed projects and datasets that are no longer currently active can be stored here.\nIt also stores data on magnetic tape, but on two redundant copies in the same physical location. Like with /tapebackup, access is slow.\nThis system also includes a seqdata directory where data from /seqdata are moved after a period of 2 years. Users may request that data be moved back to /seqdata for a period of time if they need to actively the the data again for anlyses.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#summary-table",
    "href": "storage.html#summary-table",
    "title": "File systems and storage",
    "section": "Summary table",
    "text": "Summary table\n\n\n\n\n\n\n\n\n\n\nDirectory\nPurpose\nAccess\nQuota/Capacity\nNotes\n\n\n\n\n/home/FCAM/&lt;user&gt;\nPersonal home directory for code, config, small data\nCreated for each user\n25GB\nPermissions must remain private. For sharing, use /labs or /projects.\n\n\n/labs\nShared lab project space\nPI request, lab group members\n2TB (increasable)\nFor intra-lab collaboration.\n\n\n/projects\nShared inter-lab project space\nPI request, project group members\n2TB (increasable)\nFor collaboration across labs.\n\n\n/scratch\nTemporary storage for active analyses\nOpen to all users\n84TB (shared)\nFiles may be deleted after 90 days.\n\n\n/sandbox\nTemporary space (in development)\nSame as /scratch\nTBD\nNot yet active as of 7/25.\n\n\n/seqdata\nCentral store for raw/original sequencing data\nRead-only for users\nNA\nUse symlinks instead of copying. Contact CBC to store new data.\n\n\n/tmp\nNode-local temporary space\nShared per node\nSmall\nMay fill up unexpectedly. Set TMPDIR to redirect temp files.\n\n\n/local\nLarger node-local temp space\nShared per node\nLarger than /tmp\nClean up after use.\n\n\n/archive\nMedium-term storage (up to ~1 year)\nRequest via CBC, access varies\nNA\nSlower than active spaces, geospread, more expensive.\n\n\n/tapebackup\nLong-term storage (1+ years)\nRequest via CBC, access varies\nNA\nStored on single magnetic tape, not redundant, very slow access.\n\n\n/tapearchive\nVery long-term or permanent storage\nRequest via CBC, access varies\nNA\nRedundant magnetic tape storage, very slow access.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "bestpractices.html",
    "href": "bestpractices.html",
    "title": "Data storage practices",
    "section": "",
    "text": "How to be a good citizen"
  },
  {
    "objectID": "bestpractices.html#sharing-data-with-other-users",
    "href": "bestpractices.html#sharing-data-with-other-users",
    "title": "Data storage practices",
    "section": "Sharing data with other users",
    "text": "Sharing data with other users\nUsers often need to share files with others, or work collaboratively in a directory. To do this, you need to set appropriate permissions. You are not permitted to open permissions for other users to your home directory /home/FCAM/&lt;user&gt;. If you do this, your directory may be locked and you may temporarily lose access to HPC resources. To share or collaborate on work, use a directory in /labs, /projects, or /scratch that you have access to."
  },
  {
    "objectID": "transfer.html",
    "href": "transfer.html",
    "title": "Tranferring Data",
    "section": "",
    "text": "There are several recommended approaches for transferring data to and from the Mantis file system.\nEach are most suitable for different situations. We explain the usage of each tool below.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#getting-started",
    "href": "transfer.html#getting-started",
    "title": "Tranferring Data",
    "section": "Getting Started",
    "text": "Getting Started\nGlobus has excellent documentation. We recommend starting with this tutorial which contains almost everything you need to know to use Globus in addition to making note of the specifics below.\n\nAccessing the UCHC Globus Server\n\nOrganizational Login\nWhen first logging in to Globus, you will be prompted to select your organization. You should select the University of Connecticut. If you are not already logged in, you will be prompted to enter your UConn NetID and password.\n\n\nUCHC Globus Server Collection\nTo access the Mantis file system, search for and select, the UCHC Globus Server in the collection text field. Upon selecting the UCHC Globus Server, you will be prompted to enter your Mantis username and password.\n\n\n\nFile Paths\nNavigation to your home directory mirrors the cluster file system. You can access your home directory by entering /home/FCAM/&lt;user&gt; in the path text field.\nNavigation to other directories has one minor but unintuitive difference. The root of the cluster file system is /globus instead of /. So, for example, to access the labs directory you would enter /globus/labs in the Path text field.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#transferring-files-locally",
    "href": "transfer.html#transferring-files-locally",
    "title": "Tranferring Data",
    "section": "Transferring Files Locally",
    "text": "Transferring Files Locally\nIn order to transfer files to a local computer, you will need to install the Globus Connect Personal application.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#sharing-files",
    "href": "transfer.html#sharing-files",
    "title": "Tranferring Data",
    "section": "Sharing Files",
    "text": "Sharing Files\nFor a guide on sharing files with Globus, see this tutorial.\n\n\n\n\n\n\nNote\n\n\n\nWhen adding permissions in step 6, you generally will not need to modify the Path text field. The / in this case refers to the root of the collection and not the cluster file system.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#connecting",
    "href": "transfer.html#connecting",
    "title": "Tranferring Data",
    "section": "Connecting",
    "text": "Connecting\nTo connect to the Mantis file system using SMB, you will need to follow the instructions below for your specific operating system.\n\n\nWindows 11Windows 10MacOs\n\n\n\nConnect to the CAM VPN if not on a UConn or UCHC secure WiFi network.\nOpen File Explorer from the taskbar or the Start menu.\nSelect This PC from the left pane. Then, on the File Explorer ribbon, select More &gt; Map network drive.\n\n\n\nIn the Drive list, select a drive letter. (Any available letter will do.)\nIn the Folder box, enter the SMB server URL and path you want to mount from the table above. To connect every time you sign in to your PC, select Reconnect at sign-in.\nSelect Finish.\nWhen prompted for a username and password, enter your username as CAM&lt;user&gt; and enter your CAM password.\n\n\n\n\n\nConnect to the CAM VPN if not on a UConn or UCHC secure WiFi network.\nOpen File Explorer from the taskbar or the Start menu\nSelect This PC from the left pane. Then, on the Computer tab, select Map network drive.\n\n\n\nIn the Drive list, select a drive letter. (Any available letter will do.)\nIn the Folder box, enter the SMB server URL and path you want to mount from the table above. To connect every time you sign in to your PC, select Reconnect at sign-in.\nSelect Finish.\nWhen prompted for a username and password, enter your username as CAM&lt;user&gt; and enter your CAM password.\n\n\n\n\n\nConnect to the CAM VPN if not on a UConn or UCHC secure WiFi network.\nOpen Finder and select Go:Connect to Server from the top menu.\nEnter the address of the SMB server URL and path you want to connect to.\nIn order to automatically mount the SMB share, navigate to System Settings &gt; General &gt; Login Items & Extensions.\nUnder the Open at Login section, click the + button and\n\n\n\nSelect the mounted SMB share on the sidebar and then the directory or directories in your finder window.\n\n\n\nWhen prompted for a username and password, enter your CAM username and password.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "permissions.html",
    "href": "permissions.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "File permissions"
  },
  {
    "objectID": "running.html",
    "href": "running.html",
    "title": "Job Submission",
    "section": "",
    "text": "The submit nodes on the cluster should only be used for a limited range of tasks that do not have high computational demands. See the best practices page for more information on appropriate use of the submit nodes. When in doubt it is better to complete a task as a job on a compute node.\nThe UCHC Computer cluster uses SLURM for managing and scheduling jobs.\nWith SLURM there are two primary ways that a job can be run on a compute node.",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "running.html#general",
    "href": "running.html#general",
    "title": "Job Submission",
    "section": "general",
    "text": "general\nThis partition contains nodes suitable for most tasks. These nodes have 64-128 AMD or Intel Xeon CPUs and 192 GB - 512 GB of ram. Most nodes in this partition have either one NVIDIA A100 or three NVIDIA L40S GPUs. Users can utilize up to 400 CPUs concurrently or with a limit of 400 concurrent jobs in this partition.",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "running.html#himem",
    "href": "running.html#himem",
    "title": "Job Submission",
    "section": "himem",
    "text": "himem\nThis parition contains nodes suitable for jobs that have very large memory requirements. These nodes have 128 AMD CPUs and 1 TB of ram. Each node in this partition has one NVIDIA A100 GPU. Jobs cannot run in this partition unless 500 GB of memory is requested. Only two jobs per user will run concurrently in this partition.",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "running.html#debug",
    "href": "running.html#debug",
    "title": "Job Submission",
    "section": "debug",
    "text": "debug\nThis partition is intended for testing and debugging jobs. It has a time limit of only 5 minutes but jobs should start very quickly, allowing you to check for errors before submitting to a queue with potentially longer wait times.",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "running.html#preemptible",
    "href": "running.html#preemptible",
    "title": "Job Submission",
    "section": "preemptible",
    "text": "preemptible\nJobs run in this partition my be preempted by higher priority users. This partition is intended for users who are willing to have their jobs interrupted in exchange for potentially lower wait times when nodes are not in use by higher priority users. Jobs with short run times or jobs that can be restartd are good candidates for this partition.",
    "crumbs": [
      "Running Jobs"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Many commonly used bioinformatics software packages are already installed on the cluster. This page provides information on how find and use this software as well as ways to install software yourself.\n\nRequest Software\nUsers can request the installation of software by submitting a software request\n\n\nEnvironment Modules\nEnvironment modules provide a convenient way of using most of the installed sotfware on the Mantis cluster. Environment modules are a dynamic way to modify your shell environment, allowing you to easily load and unload different software packages, compilers, and libraries without conflicts. They work by setting up the necessary environment variables (like PATH, LD_LIBRARY_PATH, and MANPATH).\nBelow is a list of commonly used commands, where &lt;module&gt; is the target module.\n\n\n\nModule Command\nDescription\n\n\n\n\nmodule avail\nDisplay all available modules.\n\n\nmodule load &lt;module&gt;\nLoads a specific module into environemnt.\n\n\nmodule list\nShow list of currently loaded modules.\n\n\nmodule unload &lt;module&gt;\nUnloads a specific module.\n\n\nmodule purge\nUnloads all loaded modules.\n\n\n\nThe Mantis cluster uses the Lmod implementation of environment modules.\n\n\nSpack\nGoing forward, most software on the Mantis cluster will now be installed using Spack, a package manager designed for high-performance computing (HPC) environments. All software installed with Spack can be loaded with environment modules, but users can use spack for managing their sotware environment instead. Spack provides a flexible and powerful way to install, configure, and manage software packages and their dependencies in a reproducible manner.\n\nInitialization\nTo initialize spack, run the following command:\nsource /isg/shared/spack/share/spack/setup-env.sh\nYou could add this to your .bashrc or .bash_profile to automatically source it in future sessions.\nTo initialize spack with other shells, find the appropriate setup script in source /isg/shared/spack/share/spack/.\n\n\nLoading Software\nAlready installed software can be loaded using spack load &lt;software&gt;. To see a list of available software, run spack find.\n\n\nInstalling Software\nIf you want to install packages with Spack, you must first chain your home directory configuration to the cluster’s Spack installation. To do so run:\nspack config add upstreams:spack-instance-1:install_tree:/isg/shared/spack/opt/spack\n\n\n\nSingularity\n\nReproducing environments and managing dependencies is difficult problem. Containerization is a powerful solution to this problem, allowing users to package software and its dependencies into a single, portable unit. Singularity is a container platform that is well-suited for HPC environments and can use existing Docker images.\nSingularity is in your PATH by default so it can be run without loading a module.\nTypically you will use singularity exec to run a command inside a container like so:\nsingularity exec &lt;path to container.sif&gt; &lt;command&gt; &lt;command-args&gt;\n\n\nConda\n\nConda is a popular package and environment management system that allows users to install, run, and update software packages in isolated environments. It is particularly popular for Python users but can be used for R packages and many other languages and tools as well.\nSee the [conda-forge]https://conda-forge.org/download/ website for documentation on using conda.\n::: {. callout-waring} Some conda commands will utilize all of the available CPU resources for long periods of time on login nodes. This can negatively impact other users on these nodes. Please refrain from running conda install, conda create, or conda update on login nodes. Instead, use an interactive session or a batch job to run these commands. Any long-running, resource-intensive processes running on login nodes may be terminated without notice.\n:::"
  },
  {
    "objectID": "connect.html",
    "href": "connect.html",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "Users can connect to the Mantis cluster through a command-line interface using SSH which requires an SSH client.\n\n\nMacOS and Linux users can use the built-in SSH client through the Terminal application.\n\n\n\nSince Windows 10, Windows comes with a built-in SSH client which can be accessed through the Windows PowerShell application. For older versions of Windows, you can use a third-party SSH client such as MobaXterm.\n\n\n\n\n\n\nWarning\n\n\n\nWhen connecting to Mantis over SSH, you will be connected to a login node, also known as a head or submit node. Please do not run anything on these nodes that is computationally demanding as this can negatively impact other users who are also connecting to these nodes. To do real work, submit a batch job or initiate an interactive session. Long running or intensive processes runnong on login nodes may be killed without notice. This includes some software installations such as when using Conda or compiling code. When in doubt, it is best to submit a batch job or initiate an interactive session.\n\n\n\n\n\nBefore being able to login with your SSH client, you will need to setup SSH keys. SSH keys are a secure authentication method that can be used for remote access to a server without using passwords. It is in fact a more secure authentication method than using a password. SSH keys consist of a pair of cryptographic keys — a private key kept secret on your computer and a public key placed on the remote server you want to access.\n\n\nYou can create a public and private SSH key pair using your SSH client in Windows PowerShell or a MacOS / Linux Terminal.\n\nFirst check for an existing SSH key pair by running the following command:\n\n\nWindows PowerShellMacOS / Linux Terminal\n\n\nls $env:USERPROFILE\\.ssh\\\n\n\nls ~/.ssh/\n\n\n\nIf you see files named id_ed25519.pub and id_ed25519, you already have an SSH key pair and can skip ahead to the Copy to Mantis section. If you do not see any files, you will need to create a new SSH key pair.\n\nTo create a key pair run the following command:\n\nssh-keygen -t ed25519 -C &lt;comment&gt;\nReplacing &lt;comment&gt; with something to help you identify which computer this key pair was created for. For example: laptop\n\nYou will be prompted to specify the location to save the key. You can press enter to accept the default location.\nNext you will be prompted to create a passphrase which is optional. If you don’t wish to create a passphrase, simply press enter.\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not share your private key with anyone! Your private key should be kept secret and secure on your computer. If someone else has access to your private key, they can access the Mantis cluster as you. We will never ask you for your private key for any reason.\n\n\n\n\n\nOnce you have created your SSH key pair, you will need to copy the public key to the Mantis cluster.\nIf you are not connected to a UConn or UCHC Secure WiFi network, you must first connect to the CAM VPN.\n\nMacOS and LinuxWindows PowerShell\n\n\nOn MacOS and Linux, you can copy your public key to the Mantis cluster using the ssh-copy-id command:\nssh-copy-id &lt;user&gt;@transfer.cam.uchc.edu\nWhere &lt;user&gt; is your CAM username.\nWhen prompted, enter your CAM password. Note that for security reasons, your cursor will remain stationary and will not see any characters appear as you type your password.\n\n\nOn Windows 10 or later, you can copy your public key to the Mantis cluster with Windows PowerShell using the command:\ntype $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh &lt;user&gt;@transfer.cam.uchc.edu \"cat &gt;&gt; .ssh/authorized_keys\"\nWhere &lt;user&gt; is your Mantis username.\n\n\n\n\n\n\n\nOnce you have copied your public SSH key to the Mantis cluster, you can connect using the SSH client.\nTo connect to Mantis through SSH, open a Windows PowerShell or Terminal window and enter the following command:\nssh &lt;user&gt;@login.hpc.cam.uchc.edu\nWhere &lt;user&gt; is your Mantis username.\nThe login.cam.hpc.edu host will redirect to one of three login nodes:\n\nmantis-sub-5.cam.uchc.edu\nmantis-sub-6.cam.uchc.edu\nmantis-sub-7.cam.uchc.edu",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#ssh-keys",
    "href": "connect.html#ssh-keys",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "Before being able to login with your SSH client, you will need to setup SSH keys. SSH keys are a secure authentication method that can be used for remote access to a server without using passwords. It is in fact a more secure authentication method than using a password. SSH keys consist of a pair of cryptographic keys — a private key kept secret on your computer and a public key placed on the remote server you want to access.\n\n\nYou can create a public and private SSH key pair using your SSH client in Windows PowerShell or a MacOS / Linux Terminal.\n\nFirst check for an existing SSH key pair by running the following command:\n\n\nWindows PowerShellMacOS / Linux Terminal\n\n\nls $env:USERPROFILE\\.ssh\\\n\n\nls ~/.ssh/\n\n\n\nIf you see files named id_ed25519.pub and id_ed25519, you already have an SSH key pair and can skip ahead to the Copy to Mantis section. If you do not see any files, you will need to create a new SSH key pair.\n\nTo create a key pair run the following command:\n\nssh-keygen -t ed25519 -C &lt;comment&gt;\nReplacing &lt;comment&gt; with something to help you identify which computer this key pair was created for. For example: laptop\n\nYou will be prompted to specify the location to save the key. You can press enter to accept the default location.\nNext you will be prompted to create a passphrase which is optional. If you don’t wish to create a passphrase, simply press enter.\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not share your private key with anyone! Your private key should be kept secret and secure on your computer. If someone else has access to your private key, they can access the Mantis cluster as you. We will never ask you for your private key for any reason.\n\n\n\n\n\nOnce you have created your SSH key pair, you will need to copy the public key to the Mantis cluster.\nIf you are not connected to a UConn or UCHC Secure WiFi network, you must first connect to the CAM VPN.\n\nMacOS and LinuxWindows PowerShell\n\n\nOn MacOS and Linux, you can copy your public key to the Mantis cluster using the ssh-copy-id command:\nssh-copy-id &lt;user&gt;@transfer.cam.uchc.edu\nWhere &lt;user&gt; is your CAM username.\nWhen prompted, enter your CAM password. Note that for security reasons, your cursor will remain stationary and will not see any characters appear as you type your password.\n\n\nOn Windows 10 or later, you can copy your public key to the Mantis cluster with Windows PowerShell using the command:\ntype $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh &lt;user&gt;@transfer.cam.uchc.edu \"cat &gt;&gt; .ssh/authorized_keys\"\nWhere &lt;user&gt; is your Mantis username.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#using-ssh",
    "href": "connect.html#using-ssh",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "Once you have copied your public SSH key to the Mantis cluster, you can connect using the SSH client.\nTo connect to Mantis through SSH, open a Windows PowerShell or Terminal window and enter the following command:\nssh &lt;user&gt;@login.hpc.cam.uchc.edu\nWhere &lt;user&gt; is your Mantis username.\nThe login.cam.hpc.edu host will redirect to one of three login nodes:\n\nmantis-sub-5.cam.uchc.edu\nmantis-sub-6.cam.uchc.edu\nmantis-sub-7.cam.uchc.edu",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "Getting Help\nIf you have a question that is not answered on this site, please submit a ticket here: https://bioinformatics.uconn.edu/contact-us/bioinformatics-technical-issues/\n\n\nSlack\nJoin the CBC slack channel to ask questions and receive cluster updates and notifications.",
    "crumbs": [
      "Get Help"
    ]
  },
  {
    "objectID": "quick.html",
    "href": "quick.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Accessible on campus or through VPN.\n\n\nmantis-submit.cam.uchc.edu\n\n\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#internal-login-node",
    "href": "quick.html#internal-login-node",
    "title": "UCHC HPC",
    "section": "",
    "text": "Accessible on campus or through VPN.\n\n\nmantis-submit.cam.uchc.edu\n\n\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#external-login-node",
    "href": "quick.html#external-login-node",
    "title": "UCHC HPC",
    "section": "External Login Node",
    "text": "External Login Node\nRequires SSH key.\nhpc.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#transfer-node",
    "href": "quick.html#transfer-node",
    "title": "UCHC HPC",
    "section": "Transfer Node",
    "text": "Transfer Node\ntransfer.cam.uchc.edu"
  }
]