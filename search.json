[
  {
    "objectID": "submission.html",
    "href": "submission.html",
    "title": "Batch job submission",
    "section": "",
    "text": "The submit nodes on the cluster should only be used for a limited range of tasks that do not have high computational demands. See the best practices page for more information about appropriate use of the submit nodes. When in doubt it is better to complete a task as a job on a compute node.\nThe UCHC Computer cluster uses SLURM for managing and scheduling jobs.\nWith SLURM there are two ways that a job can be run on a compute node.",
    "crumbs": [
      "Job Submission"
    ]
  },
  {
    "objectID": "submission.html#partitions",
    "href": "submission.html#partitions",
    "title": "Batch job submission",
    "section": "Partitions",
    "text": "Partitions\nThere are several partitions within the cluster. A partition is a groups of nodes that have certain capabilities.\n\ngeneral\nThis partition contains nodes suitable for most tasks. These nodes have TODO: how many cpus and how much memory.\n\n\nhimem\nThis parition contains nodes suitable for jobs that have very large memory requirements. These nodes have TODO: how many cpus and how much memory.\n\n\ngpu\nThis parition contains nodes with gpus available. These nodes have TODO: need to describe the specification of these nodes.\n\n\nvcell\nTODO: What is this partition for?",
    "crumbs": [
      "Job Submission"
    ]
  },
  {
    "objectID": "submission.html#interactive-job",
    "href": "submission.html#interactive-job",
    "title": "Batch job submission",
    "section": "Interactive Job",
    "text": "Interactive Job\nAn interactive session can be started on a compute node using the srun command.\nAt a minimum, you would use these arguments: srun --partition=general --qos=general --pty bash\nMost sbatch arguments can also be used with srun such as --mem or -c if you need to request specific resources.",
    "crumbs": [
      "Job Submission"
    ]
  },
  {
    "objectID": "submission.html#job-status-and-monitering",
    "href": "submission.html#job-status-and-monitering",
    "title": "Batch job submission",
    "section": "Job status and monitering",
    "text": "Job status and monitering\nTo monitor the status of your jobs you can run squeue -u &lt;user&gt;.",
    "crumbs": [
      "Job Submission"
    ]
  },
  {
    "objectID": "submission.html#request-time-extensions",
    "href": "submission.html#request-time-extensions",
    "title": "Batch job submission",
    "section": "Request time extensions",
    "text": "Request time extensions\nIf you have a job that may not finish within the time limit you can submit a time extension request",
    "crumbs": [
      "Job Submission"
    ]
  },
  {
    "objectID": "submission.html#job-arrays",
    "href": "submission.html#job-arrays",
    "title": "Batch job submission",
    "section": "Job Arrays",
    "text": "Job Arrays\nComing soon!",
    "crumbs": [
      "Job Submission"
    ]
  },
  {
    "objectID": "bestpractices.html",
    "href": "bestpractices.html",
    "title": "Data storage practices",
    "section": "",
    "text": "How to be a good citizen",
    "crumbs": [
      "Best Practices"
    ]
  },
  {
    "objectID": "bestpractices.html#sharing-data-with-other-users",
    "href": "bestpractices.html#sharing-data-with-other-users",
    "title": "Data storage practices",
    "section": "Sharing data with other users",
    "text": "Sharing data with other users\nUsers often need to share files with others, or work collaboratively in a directory. To do this, you need to set appropriate permissions. You are not permitted to open permissions for other users to your home directory /home/FCAM/&lt;user&gt;. If you do this, your directory may be locked down and you may temporarily lose access to HPC resources. To share or collaborate on work, use a directory in /labs, /projects, or /scratch that you have access to.\nDon’t run things on login nodes - conda - file compression - large compilations\nTest before scaling\nWarning about LLMs. Mainly, don’t blindly run output from them.",
    "crumbs": [
      "Best Practices"
    ]
  },
  {
    "objectID": "snapshots.html",
    "href": "snapshots.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Instructions on recovering files from .snapshots"
  },
  {
    "objectID": "account.html",
    "href": "account.html",
    "title": "Request an Account",
    "section": "",
    "text": "Request an Account\nIn order to access the cluster, you must first create an account.\n\nSubmit an account request ticket.\nSelect New Account Request from the Request Type dropdown menu.\nChoose HPC Cluster Account as the Account Type.\nFill in all relevant and required fields and click submit.\n\n\n\nPasswords\nUpon approval of an account request you will need to enroll in the CAM Password Manager and reset your password.\nPasswords expire every 90 days.\nAnytime you need to reset your password, you can do so in the CAM Password Manager.",
    "crumbs": [
      "Account & Password"
    ]
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Intro command line shell scripting\nFrequently used commands\nMove file permissions here as a subcategory?"
  },
  {
    "objectID": "connect.html",
    "href": "connect.html",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "In order to connect to the Mantis cluster, you must use an SSH client. See below for instructions based on your operating system.\n\n\n\n\n\n\nWarning\n\n\n\nWhen you connect to Mantis, you will be connected to a login node. Please do not do any serious computation on these nodes. To do real work, submit a batch job or request an interactive session. Long running or intensive processes found on login nodes may be killed without notice.\n\n\n\n\nIf you connect to the CAM virtual private network, the cluster can be accessed through this host url:\nmantis-submit.cam.uchc.edu\n\n\n\n\n\n\nTip\n\n\n\nThis host will drop you onto one of four login nodes, labeled mantis-sub-[1-4]. If the redirect fails, you can try logging in directly to one of them:\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu\nmantis-sub-4.cam.uchc.edu\n\nTODO: Appropriate/accurate to say something about load balancing here\n\n\nThe internal host is planned to be phased out in the near future so that only the external host explained below will be used.\n\n\n\nThe cluster can also be accessed through an external login host available from anywhere (on/off campus, with/without VPN). This host does not allow password authentication, however, so to use it you must set up an ssh key.\nThe host URL is:\nlogin.hpc.cam.uchc.edu\n\n\n\n\n\n\nTip\n\n\n\nAs above, this url will redirect you to one of these hosts:\n\nmantis-sub-5.cam.uchc.edu\nmantis-sub-6.cam.uchc.edu\nmantis-sub-7.cam.uchc.edu\n\nIf the redirect fails, you can try logging in directly to one of them.\n\n\n\n\n\n\nMac and Linux UsersWindows Users\n\n\nAn ssh client is installed by default on Mac and Linux operating sytems and can be run from the Terminal App.\n\n\nTo create a connection, enter the ssh command followed by your username and host url like so. Login with\nssh &lt;user&gt;@&lt;host&gt;\n\n\n\n\nCommand PromptPutty\n\n\nTODO: How to do this\n\n\nTODO: How to do this\n\n\n\n\n\n\n\n\n\nTo connect to the CAM VPN, follow the instructions here.\nNote that the CAM VPN is distinct from the University VPN. \n\n\n\nSSH keys are a secure authentication method that can be used for remote access to a server without needing passwords. It is in fact a more secure authentication method than using a password. SSH keys consist of a pair of cryptographic keys - a private key kept secret on your computer and a public key placed on the remote server you want to access.\n\nMac/LinuxWindows\n\n\n\n\nYou can create an SSH key pair To create an SSH key pair to use with the cluster, use the following command:\nssh-keygen -t ed25519 -C &lt;comment&gt;\nThe comment should be something to help you identify which computer this key comes from. For example &lt;user&gt;-laptop\nYou will be prompted to specify the location to save the key. You can press enter to accept the default location.\nNext you will be prompted to create a passphrase which is optional. If you don’t wish to create a passphrase, simply press enter.\n\n\n\nAfter creating a key pair or if you have an existing one that you want to use, you can copy it to the cluster with the following command:\nssh-copy-id &lt;user&gt;@mantis-submit.cam.uchc.edu\n\n\n\nTODO: How to do this on Windows\n\n\nTODO: Explain why you might need this and how to do it.\n\n\n\nTODO: Why you might need this and how to do it.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#internal-host-url",
    "href": "connect.html#internal-host-url",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "If you connect to the CAM virtual private network, the cluster can be accessed through this host url:\nmantis-submit.cam.uchc.edu\n\n\n\n\n\n\nTip\n\n\n\nThis host will drop you onto one of four login nodes, labeled mantis-sub-[1-4]. If the redirect fails, you can try logging in directly to one of them:\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu\nmantis-sub-4.cam.uchc.edu\n\nTODO: Appropriate/accurate to say something about load balancing here\n\n\nThe internal host is planned to be phased out in the near future so that only the external host explained below will be used.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#external-host-url",
    "href": "connect.html#external-host-url",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "The cluster can also be accessed through an external login host available from anywhere (on/off campus, with/without VPN). This host does not allow password authentication, however, so to use it you must set up an ssh key.\nThe host URL is:\nlogin.hpc.cam.uchc.edu\n\n\n\n\n\n\nTip\n\n\n\nAs above, this url will redirect you to one of these hosts:\n\nmantis-sub-5.cam.uchc.edu\nmantis-sub-6.cam.uchc.edu\nmantis-sub-7.cam.uchc.edu\n\nIf the redirect fails, you can try logging in directly to one of them.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#using-ssh-client",
    "href": "connect.html#using-ssh-client",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "Mac and Linux UsersWindows Users\n\n\nAn ssh client is installed by default on Mac and Linux operating sytems and can be run from the Terminal App.\n\n\nTo create a connection, enter the ssh command followed by your username and host url like so. Login with\nssh &lt;user&gt;@&lt;host&gt;\n\n\n\n\nCommand PromptPutty\n\n\nTODO: How to do this\n\n\nTODO: How to do this",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#vpn",
    "href": "connect.html#vpn",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "To connect to the CAM VPN, follow the instructions here.\nNote that the CAM VPN is distinct from the University VPN.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#sec-ssh-keys",
    "href": "connect.html#sec-ssh-keys",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "SSH keys are a secure authentication method that can be used for remote access to a server without needing passwords. It is in fact a more secure authentication method than using a password. SSH keys consist of a pair of cryptographic keys - a private key kept secret on your computer and a public key placed on the remote server you want to access.\n\nMac/LinuxWindows\n\n\n\n\nYou can create an SSH key pair To create an SSH key pair to use with the cluster, use the following command:\nssh-keygen -t ed25519 -C &lt;comment&gt;\nThe comment should be something to help you identify which computer this key comes from. For example &lt;user&gt;-laptop\nYou will be prompted to specify the location to save the key. You can press enter to accept the default location.\nNext you will be prompted to create a passphrase which is optional. If you don’t wish to create a passphrase, simply press enter.\n\n\n\nAfter creating a key pair or if you have an existing one that you want to use, you can copy it to the cluster with the following command:\nssh-copy-id &lt;user&gt;@mantis-submit.cam.uchc.edu\n\n\n\nTODO: How to do this on Windows\n\n\nTODO: Explain why you might need this and how to do it.\n\n\n\nTODO: Why you might need this and how to do it.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#ssh-tunnelling",
    "href": "connect.html#ssh-tunnelling",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "TODO: Explain why you might need this and how to do it.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "connect.html#x11-forwarding",
    "href": "connect.html#x11-forwarding",
    "title": "Connecting to Mantis",
    "section": "",
    "text": "TODO: Why you might need this and how to do it.",
    "crumbs": [
      "Connecting"
    ]
  },
  {
    "objectID": "quick.html",
    "href": "quick.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Accessible on campus or through VPN.\n\n\nmantis-submit.cam.uchc.edu\n\n\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#internal-login-node",
    "href": "quick.html#internal-login-node",
    "title": "UCHC HPC",
    "section": "",
    "text": "Accessible on campus or through VPN.\n\n\nmantis-submit.cam.uchc.edu\n\n\n\nmantis-sub-1.cam.uchc.edu\nmantis-sub-2.cam.uchc.edu\nmantis-sub-3.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#external-login-node",
    "href": "quick.html#external-login-node",
    "title": "UCHC HPC",
    "section": "External Login Node",
    "text": "External Login Node\nRequires SSH key.\nhpc.cam.uchc.edu"
  },
  {
    "objectID": "quick.html#transfer-node",
    "href": "quick.html#transfer-node",
    "title": "UCHC HPC",
    "section": "Transfer Node",
    "text": "Transfer Node\ntransfer.cam.uchc.edu"
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Hardware\nOperating System\nStatistics\nCurrent Status?\nCurrent Usage?\nAcknowledgements\nMaybe move all of this to introduction?"
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "Testing ideas for organizing this page.\nPut some info here about how databases are managed on the cluster.\n\nDatabase 1\nPut a brief description of the database here and maybe some instructions about how it is used.\n\nShow Paths \n\n\n\n\nPath 1\n\n\nPath 2\n\n\nPath 3\n\n\n\n\n\n\n\nDatabase 2\nPut a brief description of the database here and maybe some instructions about how it is used.\n\nShow Paths \n\n\n\n\n\nCol1\n\n\nCol2\n\n\n\n\nPath\n\n\nSource\n\n\n\n\nPath\n\n\nSource",
    "crumbs": [
      "Databases"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Environment Modules",
    "section": "",
    "text": "TODO: Write intro here\n\nEnvironment Modules\nInstalled software is managed with Environment modules.\nSee what software is available: module avail\nLoad software into environment:\nmodule load &lt;software&gt;\n\n\nSingularity\nTODO: How to use a singularity continer.\n\n\nConda\nTODO: How to install and use Conda.\n\n\nRequest Software\nUsers can request the installation of software by submitting a software request",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "storage.html",
    "href": "storage.html",
    "title": "File systems and storage",
    "section": "",
    "text": "The HPC offers a variety of systems for storage and archiving of data. With a few noted exceptions, all systems below are network-attached, meaning they are accessible from all compute and login nodes.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#active-work-spaces",
    "href": "storage.html#active-work-spaces",
    "title": "File systems and storage",
    "section": "Active work spaces",
    "text": "Active work spaces\nThe following spaces are suitable for use for active data analysis.\n\n/home/FCAM/&lt;user&gt;\nAll users will have a home directory when their account is created.\nThis location is primarily intended for storage of small files such as scripts, source code, configuration, software installtions, and can accomodate a small amount of data.\nQuota: 1TB, can be increased upon request.\n\n\n\n\n\n\nNote\n\n\n\nUsers are not permitted to open permissions on their home directories. If you need to share files with other users, try any of the below directories you have access to. If you open permissions on your home directory, it may be locked down.\n\n\n\n\n/labs\nThis can be created at the request of a PI.\nThis location is intended for storage of active shared project data to facilitate collaboration among members of a PI’s lab. You will need to be part of the PI’s permission group to access a lab directory.\nQuota: 2TB, can be increased upon request.\n\n\n/projects\nThis can be created at the request of a PI.\nThis location is intended for storage of active shared project data to facilitate collaboration among multiple labs on a project.\nQuota: 2TB, can be increased upon request.\n\n\n/scratch\nAnyone can create a directory here.\nThis location is intended to serve as temporary storage. This would be an ideal place for the output of intermediate files during analysis. As of last update, /scratch had 84TB of space. Please be considerate of other users of this shared resource.\n\n\n\n\n\n\nWarning\n\n\n\nIn /scratch files are subject to deletion without notice after 90 days.\n\n\n\n\n/sandbox\nThis system has identical policies to /scratch. It is in development. At the time of writing (7/25) it was not up.\n\n\n/seqdata\nThis location is used for storing raw and original sequencing data. Data are stored as read-only. You may request that raw data be stored here (all data generated by CGI is stored here by default), but CBC will have to move it there for you, as users do not have write access.\nIf you would like to move existing raw data to this location or store data from an external sequencing center, please contact us at cbcsupport@helpspotmail.com.\nDo not copy data from this directory. For convenience, consider making a symlink to files stored here. By avoiding unnecessary copying, accidental data corruption or deletion can be avoided and space will be used more efficiently on the cluster and within directories you own.\n\n\n\n\n\n\nTip\n\n\n\nA symlink, or symbolic link is a pointer to a file or directory that behaves like the real thing.\nIf you want to use data from /seqdata as if it were in one of your project directories you can create a symlink like this:\nln -s /seqdata/CGI/Fastq_Files/AwesomeWGS_May2025/ /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/\nWhere /seqdata/CGI/Fastq_Files/AwesomeWGS_May2025 is the raw data directory and /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/ is the destination. You will then have a symlink, /home/FCAM/&lt;user&gt;/AwesomeProject/rawData/AwesomeWGS_May2025 that behaves just as if the files contained had been copied without taking up all the space.\n\n\n\n\n/tmp\nEach node has it’s own local /tmp directory which can be used for temporary storage of data while running analyses. This can be desirable when an analysis would be limited by I/O operations across the network.\n/tmp is a shared space. Once an analysis is done, important data need to be moved out of this space and any other files should be removed.\n\n\n\n\n\n\nTip\n\n\n\nOn our system /tmp is pretty small and often fills up. Many programs quietly write to /tmp by default without telling users (they usually quietly clean up afterward as well). If /tmp fills up, no space left on device errors frequently result, confusing users who know they still have plenty of space in their storage quota. If you are running a program that does this, say:\ngenomeAssembler -i mySequences.fastq.gz -o myGenome.fasta\nYou can usually get it to write to another temporary directory by setting and exporting the variable TMPDIR to somewhere else before running it:\nexport TMPDIR=myNewTmpDir # this directory must exist or an error will result\n\ngenomeAssembler -i mySequences.fastq.gz -o myGenome.fasta\nYou can create your own temporary directory where you are doing the analysis, or you can use /local.\n\n\nThe system will remove contents of this directory after TODO: how long or under what conditions?\n\n\n/local\nThis is another space local to each node and shared among users. It is larger than /tmp and files should be removed immediately after analysis.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#archival-spaces",
    "href": "storage.html#archival-spaces",
    "title": "File systems and storage",
    "section": "Archival spaces",
    "text": "Archival spaces\nThese spaces are meant for archiving data for varying durations. Users can request the creation of directories they have read/write access to.\n\n/archive\nThis system is intended for medium term storage of data that is no longer being used for analysis, but which may be needed within a year or so. /archive gives relatively fast access (not as fast as the above directories). The data are securely stored, being geospread across four data centers. As such, it is relatively expensive for us to store data here. Users should not plan on permanently archiving data here.\nPIs and users can request storage space here in a directory structure that mirrors /home/FCAM, /projects and /labs to which they have read/write access.\nEventually, data stored here will be moved to tape storage. TODO: Say a bit more about the goal of archive and tape. How safe is data here? Does it need to also be stored elsewhere such as SRA?\nIf you would like to move data to this location please contact us at cbcsupport@helpspotmail.com\n\nReading from this directory is slow compared to /seqdata. Request that data be moved back to /seqdata if you will be actively using the data for analysis.\n\n\n\n/tapebackup\nThis system is intended for longer-term storage (greater than one year). This system stores data on magnetic tape. Data stored here goes on a single magnetic tape and is not backed up, though magnetic tape is highly stable.\nAccess to magnetic tape is slow. Depending on how busy the system is, it may take hours or days for data to be written or retrieved. It is, however, inexpensive.\nThe directory structure is similar to /archive. Users can request space here to which they have read/write access.\n\n\n/tapearchive\nThis system is intended for very long term to permanent storage. It also stores data on magnetic tape, but on two redundant systems. Like /tapebackup, access is slow.\nUser access and directory structure are the same as /archive and /tapebackup.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "storage.html#summary-table",
    "href": "storage.html#summary-table",
    "title": "File systems and storage",
    "section": "Summary table",
    "text": "Summary table\n\n\n\n\n\n\n\n\n\n\nDirectory\nPurpose\nAccess\nQuota/Capacity\nNotes\n\n\n\n\n/home/FCAM/&lt;user&gt;\nPersonal home directory for code, config, small data\nCreated for each user\n1TB (increasable)\nPermissions must remain private. For sharing, use /labs or /projects.\n\n\n/labs\nShared lab project space\nPI request, lab group members\n2TB (increasable)\nFor intra-lab collaboration.\n\n\n/projects\nShared inter-lab project space\nPI request, project group members\n2TB (increasable)\nFor collaboration across labs.\n\n\n/scratch\nTemporary storage for active analyses\nOpen to all users\n84TB (shared)\nFiles may be deleted after 90 days.\n\n\n/sandbox\nTemporary space (in development)\nSame as /scratch\nTBD\nNot yet active as of 7/25.\n\n\n/seqdata\nCentral store for raw/original sequencing data\nRead-only for users\nNA\nUse symlinks instead of copying. Contact CBC to store new data.\n\n\n/tmp\nNode-local temporary space\nShared per node\nSmall (varies by node)\nMay fill up unexpectedly. Set TMPDIR to redirect temp files.\n\n\n/local\nLarger node-local temp space\nShared per node\nLarger than /tmp\nClean up after use.\n\n\n/archive\nMedium-term storage (up to ~1 year)\nRequest via CBC, access varies\nNA\nSlower than active spaces, geospread, more expensive.\n\n\n/tapebackup\nLong-term storage (1+ years)\nRequest via CBC, access varies\nNA\nStored on single magnetic tape, not redundant, very slow access.\n\n\n/tapearchive\nVery long-term or permanent storage\nRequest via CBC, access varies\nNA\nRedundant magnetic tape storage, very slow access.",
    "crumbs": [
      "File Systems & Storage"
    ]
  },
  {
    "objectID": "transfer.html",
    "href": "transfer.html",
    "title": "Tranferring Data",
    "section": "",
    "text": "We recommend two approaches for transferring data to and from the cluster.\nEach are most suitable for different situations. We explain the usage of each tool below.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#secure-copy",
    "href": "transfer.html#secure-copy",
    "title": "Tranferring Data",
    "section": "Secure Copy",
    "text": "Secure Copy\nTODO: Are all network storage filesystems mounted to the transfer nodes?\n\nSecure copy is ideal for transferring small amounts of data and is a bit more straight forward to use than Globus connect. A secure copy can be performed with the scp command. This command can only be used to transfer data to and from a transfer node. There are two transfer nodes.\n\ntransfer.cam.uchc.edu This will accept password authentication or ssh keys and is available from anywhere.\ntransfer.hpc.cam.uchc.edu This is actually three load-balanced servers. it will only accept SSH key authentication and can only be reached through the CAM VPN.\n\nA secure copy can be performed from the terminal with the scp command which has this required syntax:\nscp &lt;src&gt; &lt;dest&gt;\nThe src and dest arguments specify file or directory paths.\nAny remote file path must be prefixed with a user ID along with the transfer node address and separated from the path with a colon like so:\n&lt;user&gt;@&lt;hostname&gt;:.\nIn order to secure copy a directory use the -r option.\n\nExamples\nCopy a file from your local computer to your home directory on the cluster:\nscp &lt;local path&gt; &lt;user&gt;@transfer.cam.uchc.edu:&lt;remote path&gt;\nCopy a directory from the cluster to your local computer:\nscp -r &lt;user&gt;@transfer.cam.uchc.edu:&lt;remote path&gt; &lt;local path&gt;",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "transfer.html#globus-connect",
    "href": "transfer.html#globus-connect",
    "title": "Tranferring Data",
    "section": "Globus Connect",
    "text": "Globus Connect\nGlobus Connect is well suited for large file transfers and for transferring between institutions such as when you need to share files with a collaborator.\n\nConnecting with Globus Connect\n\nLog in to the Globus web app at https://www.globus.org.\nSelect the University of Connecticut as your organization and enter your login credentials when prompted.\nOnce logged in, connect to UConn HPC by searching for “UCHC Globus Server” in the collection text field.\nSelect UCHC Globus Server and log in with your CAM credentials.\n\n\n\n\n\n\n\nNote\n\n\n\nGlobus Connect also has a desktop client available for installation on your local computer.\nThis can be downloaded at https://www.globus.org/globus-connect-personal\n\n\n\n\nFinding files in UCHC Globus Server collection\nThere are two root directories within the globus server collection. You can navigate the server file system by entering paths within these two directories into the path text field.\n\n/globus\nThis Globus file server directory contains many of the directories located in root directory of the cluster file system. From here you can access the /labs, /projects, or /seqdata cluster file system directories among others.\n\n\n/home\nThis Globus file server directory contains the /home cluster file system directory. You would access your home directory by navigating to /home/FCAM/&lt;user&gt; in the Globus file server.",
    "crumbs": [
      "Transferring Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UConn Health High Performance Computing",
    "section": "",
    "text": "This site provides documentation for users of the Mantis computer cluster, housed at UConn Health. This cluster is available to all UConn researchers, with a focus on bioinformatics, genomics and other data-intensive computational problems.\nThis page is maintained by the Computational Biology Core at the Institute for Systems Genomics.\nThere is a second high performance computing facility (also available to all UConn researchers) housed at Storrs campus. To learn more about accessing and using that system, visit this site."
  },
  {
    "objectID": "permissions.html",
    "href": "permissions.html",
    "title": "UCHC HPC",
    "section": "",
    "text": "File permissions"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "Getting Help\nIf you have a question that is not answered on this site, please submit a ticket here: https://bioinformatics.uconn.edu/contact-us/bioinformatics-technical-issues/\n\n\nSlack\nJoin the cbc slack channel to ask questions and receive updates and notifications.",
    "crumbs": [
      "Get Help"
    ]
  }
]